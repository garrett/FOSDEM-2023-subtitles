1
00:00:00,000 --> 00:00:10,840
So, it's nice to see a nice crowd after two years of pandemic.

2
00:00:10,840 --> 00:00:12,560
You're beautiful.

3
00:00:12,560 --> 00:00:20,720
So today we're going to talk about similarity detection and how we use it in integrity.

4
00:00:20,720 --> 00:00:31,440
As a way to ensure that the website is a safe place, that people just maintain an integrity

5
00:00:31,440 --> 00:00:33,840
of place.

6
00:00:33,840 --> 00:00:36,120
The outline of the presentation is as follows.

7
00:00:36,120 --> 00:00:41,640
We're going to outline the problem, then how we use automation and similarity detection

8
00:00:41,640 --> 00:00:45,120
in order to achieve what we want.

9
00:00:45,120 --> 00:00:50,120
The current technology that we use for images, which is the vector search, then we are going

10
00:00:50,120 --> 00:00:57,600
to discuss in depth what is the actual technology, the vector embedding that makes possible to

11
00:00:57,600 --> 00:01:01,360
transform a picture into an element of search.

12
00:01:01,360 --> 00:01:06,760
The current platform offering that met up with this proposal to allow other people to

13
00:01:06,760 --> 00:01:12,840
crowd source all of their findings into a centralized place.

14
00:01:12,840 --> 00:01:19,000
And last but not least, what we have of open and free that you can install in your own,

15
00:01:19,000 --> 00:01:27,520
you can deploy in your own site to benefit all these technological findings.

16
00:01:27,520 --> 00:01:34,240
So the problem is that any big platform bears the responsibility to ensure it's a safe place

17
00:01:34,240 --> 00:01:35,240
to serve.

18
00:01:35,240 --> 00:01:40,640
No matter what also the law says, that you have to make sure whatever the user posts,

19
00:01:40,640 --> 00:01:47,280
you are ultimately responsible to make sure that everybody is just not exposed to things

20
00:01:47,280 --> 00:01:52,080
that will violate your community guidelines.

21
00:01:52,080 --> 00:01:57,520
Meta has almost three billion users, it's likely less than a world population.

22
00:01:57,520 --> 00:02:03,240
And although the vast majority of our users follow rules, some fringe bad actors will

23
00:02:03,240 --> 00:02:04,520
always be present.

24
00:02:04,520 --> 00:02:13,800
And at that scale, fringe means tens of millions of bad person creating a lot of problems.

25
00:02:13,800 --> 00:02:21,800
And when I mean issues, problems, I mean child exploitation, imageries, non-consensual intimate

26
00:02:21,800 --> 00:02:27,440
imagery, which is a way to say revenge porn, adult sexual exploitation, people forced to

27
00:02:27,440 --> 00:02:36,360
perform sexual acts in front of camera against their will, terrorism, violence, whatever.

28
00:02:36,360 --> 00:02:43,200
And just to give you a couple of numbers, Meta publishes a transparency report quarterly

29
00:02:43,200 --> 00:02:48,840
about what we do to ensure the platform stays safe.

30
00:02:48,840 --> 00:02:57,360
And on the second quarter of 2022, we removed the 38 million of adult sexual exploitation

31
00:02:57,360 --> 00:02:59,280
pieces of content taken down.

32
00:02:59,280 --> 00:03:03,760
And it's just for this category, child exploitation is not so huge, thank God, but also there

33
00:03:03,760 --> 00:03:07,200
are other like violence, terrorism and stuff.

34
00:03:07,200 --> 00:03:12,680
That accounted for the 0.04% of view content worldwide.

35
00:03:12,680 --> 00:03:20,640
And in case you were asking, 97% of this content was proactively taken off, even before people

36
00:03:20,640 --> 00:03:23,840
could even see it.

37
00:03:23,840 --> 00:03:29,320
The remaining 2.8% is user reports, like I found this.

38
00:03:29,320 --> 00:03:34,440
And we take that down also, and we also add to the data banks just to make sure that we

39
00:03:34,440 --> 00:03:36,840
are not forgetting about that.

40
00:03:36,840 --> 00:03:40,280
Sometimes there are false positives because it's just unavoidable.

41
00:03:40,280 --> 00:03:44,000
And half million was restored upon user appeal.

42
00:03:44,000 --> 00:03:51,080
And we restore accounts and mostly accounts and the pictures that we're banned for.

43
00:03:51,080 --> 00:03:58,000
It goes by itself to the sheer volume of content, the huge scale, the problem we are facing,

44
00:03:58,000 --> 00:04:05,040
requires both automation and also human review to ensure either accuracy, both accuracy and

45
00:04:05,040 --> 00:04:06,540
also consistency.

46
00:04:06,540 --> 00:04:11,520
So there will be a problem if we had the 1 million people clicking and making decisions

47
00:04:11,520 --> 00:04:16,440
and what is violating for one is not for the other and vice versa.

48
00:04:16,440 --> 00:04:21,480
And so, and we cannot just also just employ automation, because otherwise we will have

49
00:04:21,480 --> 00:04:27,840
this very powerful site, decapitating everybody, also innocent users.

50
00:04:27,840 --> 00:04:34,600
So the role of automation and similarity detection, the thing is that a lot of things that happen

51
00:04:34,600 --> 00:04:38,360
online are things that are being repeated.

52
00:04:38,360 --> 00:04:40,560
So are things that are already occurred in the past.

53
00:04:40,560 --> 00:04:47,560
Like people posting a picture of some shooting, some mass shooting, for example, like the

54
00:04:47,560 --> 00:04:53,800
buffalo or the Christ church, gets taken down and the 10 more accounts spawn and post the

55
00:04:53,800 --> 00:04:54,800
same things.

56
00:04:54,800 --> 00:05:03,760
So it's much, it's very efficient to reason in terms of let's just redo the things that

57
00:05:03,760 --> 00:05:06,600
we already found out that worked.

58
00:05:06,600 --> 00:05:11,680
We employ automation to scale, of course, handle the scale of the problem and to consistently

59
00:05:11,680 --> 00:05:17,280
repeat a decision that a human reviewer has already vetted in the past.

60
00:05:17,280 --> 00:05:24,080
So we tie a content to a decision, a violating content to a decision, let's act upon this.

61
00:05:24,080 --> 00:05:27,840
And we tie the decision to the actions.

62
00:05:27,840 --> 00:05:33,160
Let's just repeat this action every time we meet a piece of content that triggered this

63
00:05:33,160 --> 00:05:34,640
same decision.

64
00:05:34,640 --> 00:05:38,680
We do that for videos, for pictures, and also for text.

65
00:05:38,680 --> 00:05:44,360
Today we'll be mostly talking about images because the techniques for video and pictures

66
00:05:44,360 --> 00:05:46,320
are somewhat very similar.

67
00:05:46,320 --> 00:05:53,280
Text has a completely different array of techniques that we'll not be presenting today.

68
00:05:53,280 --> 00:05:59,440
So a way to, if you want to achieve similarity detection, you have to come up with a way

69
00:05:59,440 --> 00:06:02,040
to achieve similarity first.

70
00:06:02,040 --> 00:06:05,240
So how do we compare to pictures?

71
00:06:05,240 --> 00:06:09,400
Of course, we are not, we are not be doing pixel by pixel comparison.

72
00:06:09,400 --> 00:06:11,640
We want to be much faster.

73
00:06:11,640 --> 00:06:16,960
Our way to do that is just, okay, let's just MD5 hash all the pictures or SHA-1 all the

74
00:06:16,960 --> 00:06:23,200
pictures and then we store them somewhere in an indexing system and whenever a new picture

75
00:06:23,200 --> 00:06:31,080
comes in, we just recreate the hash and if it matches, we just ban, right?

76
00:06:31,080 --> 00:06:36,360
Well, that doesn't work very well because the cryptographic hashes are not resistant

77
00:06:36,360 --> 00:06:43,640
to resizing, rotation, one pixel alteration, all the hash changes all together.

78
00:06:43,640 --> 00:06:51,680
Instead, we can really benefit from local hashing because it allows for similarity measurement.

79
00:06:51,680 --> 00:06:58,960
Like you change slightly one piece, one portion of the image and the hash changes a little,

80
00:06:58,960 --> 00:06:59,960
but not completely.

81
00:06:59,960 --> 00:07:06,200
Then you can reason in terms of distance between two hashes.

82
00:07:06,200 --> 00:07:10,320
So you have to turn, you have to find a way to turn an image into a vector and then you

83
00:07:10,320 --> 00:07:12,000
perform a vector search.

84
00:07:12,000 --> 00:07:18,320
Whenever two vectors are very, very close beyond a certain threshold, then it's probably

85
00:07:18,320 --> 00:07:19,720
a match.

86
00:07:19,720 --> 00:07:24,000
And just in case if you're asking, this is your base as the architecture.

87
00:07:24,000 --> 00:07:30,240
You have more or less all the architectures share these four stages, observation, an image

88
00:07:30,240 --> 00:07:35,400
has been generated, usually push event like user uploaded something.

89
00:07:35,400 --> 00:07:42,360
Then you have the representation phase in which you hash the image to a compact representation.

90
00:07:42,360 --> 00:07:48,000
If you're indexing, you store that into your index and instead if you are at inference time

91
00:07:48,000 --> 00:07:55,200
like an event someone uploaded something, you search the index they have built with representation.

92
00:07:55,200 --> 00:08:03,160
In case you have a match, you action upon what you decide what to do with the match you got.

93
00:08:03,160 --> 00:08:07,120
Usually the idea is that this is very close to an image that I already see in the past

94
00:08:07,120 --> 00:08:10,200
that was banned and also the account was taken down.

95
00:08:10,200 --> 00:08:14,480
Do the same to this user.

96
00:08:14,480 --> 00:08:24,480
So first three pieces of content, Facebook has released a library which is FICE, the

97
00:08:24,480 --> 00:08:32,320
Facebook similarity search library is a library to do similarity search over a vector of dense

98
00:08:32,320 --> 00:08:36,400
vectors or vector floats or integers, for example.

99
00:08:36,400 --> 00:08:42,600
You can think about it like a C++ version of Lucene so you index stuff, puts that in

100
00:08:42,600 --> 00:08:46,400
a very big space and you can search in this space very fast.

101
00:08:46,400 --> 00:08:51,720
It supports CUDA so you can use your GPUs to search.

102
00:08:51,720 --> 00:08:58,000
It's basically index on steroids and it's C++ but it has Python bindings available and

103
00:08:58,000 --> 00:09:00,120
it scales almost nearly.

104
00:09:00,120 --> 00:09:07,400
You can really index 100 millions of pieces on a single machine and it just handles them

105
00:09:07,400 --> 00:09:14,560
really, doesn't need to saturate all the memory so it has a very good optimization properties

106
00:09:14,560 --> 00:09:16,680
that makes it very good too.

107
00:09:16,680 --> 00:09:21,480
And you can go and download that on GitHub.

108
00:09:21,480 --> 00:09:27,920
Today we are also mostly referring to with the perceptual ashing.

109
00:09:27,920 --> 00:09:33,240
This means that we are reasoning in terms of colors, colors and images, shapes.

110
00:09:33,240 --> 00:09:36,680
We are not reasoning about what's happening inside the image.

111
00:09:36,680 --> 00:09:43,200
That's the semantic ashing which we are not going to talk about this today.

112
00:09:43,200 --> 00:09:49,000
Perceptual ashing just captures visual similarities and it's very nice for use case because it

113
00:09:49,000 --> 00:09:52,120
exactly does its job.

114
00:09:52,120 --> 00:09:58,960
So you might think that we are all talking about machine learning systems that come up

115
00:09:58,960 --> 00:10:04,880
with very clever representations about our pictures and I'm asking do we really need

116
00:10:04,880 --> 00:10:06,800
a convnet for that?

117
00:10:06,800 --> 00:10:09,480
Do we really need to employ GPUs?

118
00:10:09,480 --> 00:10:15,120
You already said that it's on CUDA so perhaps that's a nice hint but absolutely not.

119
00:10:15,120 --> 00:10:21,240
Most of this technology is like a ashing technology so they just computer represent a mathematical

120
00:10:21,240 --> 00:10:26,760
transformation over the image and it's really fast and it's really cheap and it can be executed

121
00:10:26,760 --> 00:10:29,080
almost everywhere.

122
00:10:29,080 --> 00:10:37,520
So a little bit of history, the first very notable example, it comes from a source that

123
00:10:37,520 --> 00:10:42,400
nobody would have thought about, it's Microsoft in 2009.

124
00:10:42,400 --> 00:10:44,400
Microsoft invents photo DNA.

125
00:10:44,400 --> 00:10:50,640
Photo DNA is the first algorithm employed in fight against exploitive images of children.

126
00:10:50,640 --> 00:11:04,080
So it transforms a picture into an ash of 144 unsigned integers on 8-bit representation.

127
00:11:04,080 --> 00:11:05,720
It's proprietary.

128
00:11:05,720 --> 00:11:15,800
So Microsoft licenses this to any non-profit or any organization that wants to fight exploitive

129
00:11:15,800 --> 00:11:16,800
images of children.

130
00:11:16,800 --> 00:11:19,560
It gives you a license, you can use for that and nothing else.

131
00:11:19,560 --> 00:11:23,080
But I cannot disclose the details of how that works.

132
00:11:23,080 --> 00:11:29,040
It can be used only for that but Microsoft donated the photo DNA to the National Center

133
00:11:29,040 --> 00:11:32,080
for the Missing and Exploited Children, the NACMAC.

134
00:11:32,080 --> 00:11:39,720
It's this American non-profit that basically acts as a coordination center in global fight

135
00:11:39,720 --> 00:11:47,880
against this phenomenon and shares this library with anyone that wants to integrate.

136
00:11:47,880 --> 00:11:53,560
This I cannot talk about how this works, this is the only moment in which I will say something

137
00:11:53,560 --> 00:11:54,560
like that.

138
00:11:54,560 --> 00:12:00,360
But we can talk about an open source counterpart that almost 10 years later Facebook releases

139
00:12:00,360 --> 00:12:01,360
PDQ.

140
00:12:01,360 --> 00:12:08,200
PDQ stands for Perceptual Algorithm Using Discrete Cousin Transform and gives a quality

141
00:12:08,200 --> 00:12:09,200
metric.

142
00:12:09,200 --> 00:12:13,560
It's a very, very bad acronym but we need a three-letter acronym so it's that.

143
00:12:13,560 --> 00:12:20,640
It creates a 256-bit hash, uses hamming distance to compute the distance.

144
00:12:20,640 --> 00:12:21,940
It's really fast.

145
00:12:21,940 --> 00:12:27,640
The compute overhead is negligible compared to discrete.

146
00:12:27,640 --> 00:12:30,920
Can tolerate some level of adversality.

147
00:12:30,920 --> 00:12:35,800
This means that you change the image because you want to fool the systems in that this

148
00:12:35,800 --> 00:12:41,840
image is not something which is well-known, PDQ can resist a little to this manipulation

149
00:12:41,840 --> 00:12:43,880
but not all of them.

150
00:12:43,880 --> 00:12:47,200
It's used in stopncii.org.

151
00:12:47,200 --> 00:12:54,120
It's a website where people, in case you have a fight with your ex-fiancÃ© and he's threatening

152
00:12:54,120 --> 00:13:02,320
to publish your intimate imagery, you go to stopncii.org, you upload your intimate imageries,

153
00:13:02,320 --> 00:13:10,000
fingerprints get taken, original images get deleted right away of course, and these fingerprints

154
00:13:10,000 --> 00:13:17,360
are shared with partners that, okay, if I am going to see these fingerprints in my website,

155
00:13:17,360 --> 00:13:19,600
my platform, I'm going to take them down.

156
00:13:19,600 --> 00:13:24,640
So it's a crowd source effort and uses PDQ for images.

157
00:13:24,640 --> 00:13:26,400
How does that work?

158
00:13:26,400 --> 00:13:33,760
So PDQ hashing is, optionally scale down to a square image, okay.

159
00:13:33,760 --> 00:13:35,880
Then you compute the luminance.

160
00:13:35,880 --> 00:13:42,080
Luminance is the idea that you take the pixel that contributes most in the RGB channel.

161
00:13:42,080 --> 00:13:45,080
Instead of putting black and white, you use the luminance.

162
00:13:45,080 --> 00:13:50,040
It's just another procedure and the idea is that luminance gives you better information

163
00:13:50,040 --> 00:13:58,240
about what was the channel that was contributing most to the color, to the light in that place.

164
00:13:58,240 --> 00:14:04,000
Then you down sample to 64 times 64 using a blur filter.

165
00:14:04,000 --> 00:14:10,040
And the idea of the blur filter, a tent filter, is that it gets the most significant value

166
00:14:10,040 --> 00:14:15,920
in that region because if you keep convoluting a pixel with your neighborhood, what you will

167
00:14:15,920 --> 00:14:19,960
have in the end will be the highest value.

168
00:14:19,960 --> 00:14:26,600
So you obtain a representation which is compact and retains the most significant information.

169
00:14:26,600 --> 00:14:32,120
Then you divide the images in 16 times 16 boxes, each one by 4 pixels, and you calculate

170
00:14:32,120 --> 00:14:35,200
a discrete cosine transform of each box.

171
00:14:35,200 --> 00:14:40,720
The discrete cosine transform, so the box is at the 4 bar color there.

172
00:14:40,720 --> 00:14:47,120
You see that the grid with a lot of wobbly images, that is a discrete cosine transform.

173
00:14:47,120 --> 00:14:53,080
The idea is that any image, any signal can be represented as a sum of cosines, sum of

174
00:14:53,080 --> 00:14:54,680
cosine signals.

175
00:14:54,680 --> 00:14:57,520
You only take the signal, the most significant one.

176
00:14:57,520 --> 00:14:59,920
So it's a form of compression actually.

177
00:14:59,920 --> 00:15:09,120
And you take the most significant coefficient for the biggest cosine you have.

178
00:15:09,120 --> 00:15:13,280
And then you calculate if the median is above a certain value, then it's one, otherwise

179
00:15:13,280 --> 00:15:14,280
it's zero.

180
00:15:14,280 --> 00:15:20,840
So you get this 256 in an array of 010101 in case this pixel were a high luminance or

181
00:15:20,840 --> 00:15:23,240
a low luminance.

182
00:15:23,240 --> 00:15:29,480
The DCT provides a spectral hashing property, identifies what is the point in the images,

183
00:15:29,480 --> 00:15:31,720
that contributes more or less.

184
00:15:31,720 --> 00:15:36,360
You have an hashing space, which is 2 to the power of 1 to 28, because it's half the

185
00:15:36,360 --> 00:15:41,240
ashes, because half is always 0, half is always 1.

186
00:15:41,240 --> 00:15:47,840
To search, you just do a vector search again, what you've just created.

187
00:15:47,840 --> 00:15:53,160
In case we want, we can use partially the same technology to do video hashing.

188
00:15:53,160 --> 00:15:57,920
And this is another, it comes in almost the same paper.

189
00:15:57,920 --> 00:16:01,160
The TMK is a temporary matching kernel.

190
00:16:01,160 --> 00:16:10,640
Is a way to use the PDQ creation to do a video similarity detection algorithm.

191
00:16:10,640 --> 00:16:12,960
It produces a fixed length video hashes.

192
00:16:12,960 --> 00:16:20,640
So your hashes stays the same length, which is like 256 kilobytes, if I'm not wrong.

193
00:16:20,640 --> 00:16:25,760
Even if your video lasts for three hours or 30 seconds, it just produces a fixed length.

194
00:16:25,760 --> 00:16:27,600
So it's really nice.

195
00:16:27,600 --> 00:16:30,560
What you do is that you resample a video to 15 frames.

196
00:16:30,560 --> 00:16:36,880
Then you compute the PDQ without the 01 quantization, so you keep the float numbers.

197
00:16:36,880 --> 00:16:40,360
That's why it's called PDQF, PDQ float.

198
00:16:40,360 --> 00:16:47,920
And then you compute the average of the old descriptors that you have within various periods

199
00:16:47,920 --> 00:16:49,600
of the cousin and scene.

200
00:16:49,600 --> 00:16:52,400
Why we add the cousin curves?

201
00:16:52,400 --> 00:16:59,920
Because a cousin or a scene adds out this wobbly movement that tells you whether a frame

202
00:16:59,920 --> 00:17:06,520
is before or later in the near surroundings, the near neighborhood of the frames.

203
00:17:06,520 --> 00:17:10,800
So in case you have like 10 pictures, you add this cousin signal.

204
00:17:10,800 --> 00:17:16,040
You know this picture is before this one, because you see the cousin curve, which is

205
00:17:16,040 --> 00:17:17,640
going up and going down.

206
00:17:17,640 --> 00:17:25,360
And it's a nice uniqueness fingerprinting time signature algorithm to add a cousin.

207
00:17:25,360 --> 00:17:31,080
So you compute the average of all the frames, the PDQF for all the frames, with various

208
00:17:31,080 --> 00:17:33,160
periods, various scene and cousin.

209
00:17:33,160 --> 00:17:38,400
And then you pack them all together, and you have these like five or six averages.

210
00:17:38,400 --> 00:17:42,800
And that's your PDQF embedding.

211
00:17:42,800 --> 00:17:48,720
Everything is just you compare first the vector zero, which is the average of all the frames

212
00:17:48,720 --> 00:17:51,880
and doesn't retain a temporal signature.

213
00:17:51,880 --> 00:17:57,240
Then if there is a match, you compare also all the other vectors at different periods,

214
00:17:57,240 --> 00:18:01,160
which are the level two action as the time signature.

215
00:18:01,160 --> 00:18:04,560
And so you can be really be sure that the videos are really the same.

216
00:18:04,560 --> 00:18:09,600
Because if you find the same averages with the same periods, it must be the same video.

217
00:18:09,600 --> 00:18:13,720
It's nice that it's resistant to resampling, because you always resample.

218
00:18:13,720 --> 00:18:19,760
So in some way, if you vary the frame rate, the video will change, and MD5 ash will change,

219
00:18:19,760 --> 00:18:22,120
but this one is not full.

220
00:18:22,120 --> 00:18:28,680
Ashing is really slow, because you have to do a transcoding of all the videos first,

221
00:18:28,680 --> 00:18:33,440
and then you have to read all the frames and compute the PDQ for every frame.

222
00:18:33,440 --> 00:18:36,640
But search is actually very fast.

223
00:18:36,640 --> 00:18:39,840
Another nice hashing technique that we have is the video MD5.

224
00:18:39,840 --> 00:18:43,560
I said that we will not be using a crypto ashes highlight.

225
00:18:43,560 --> 00:18:46,400
We use crypto ashes, but just for videos.

226
00:18:46,400 --> 00:18:51,600
This because if you take MD5 of video and find exact copies, it's really cheap in this way.

227
00:18:51,600 --> 00:18:58,240
A lot of actors just repost unmodified content.

228
00:18:58,240 --> 00:19:05,000
They are not going really through the hassle of doing encoding just to try to fool the systems.

229
00:19:05,000 --> 00:19:07,240
They just try to repost again.

230
00:19:07,240 --> 00:19:12,720
So the MD5 actually works, and it can be done with vector search, if we use the bytes for

231
00:19:12,720 --> 00:19:14,560
the MD5 algorithm.

232
00:19:14,560 --> 00:19:20,840
And it's used widely in stopncii.org also.

233
00:19:20,840 --> 00:19:27,440
In 2022, Facebook has released the video PDQ, which is a different algorithm from the former

234
00:19:27,440 --> 00:19:28,840
one.

235
00:19:28,840 --> 00:19:33,800
Hashing is that we hash every frame to a PDQ ash, and we just pack the list.

236
00:19:33,800 --> 00:19:36,080
It's much bigger.

237
00:19:36,080 --> 00:19:39,640
It's not slower than the other one.

238
00:19:39,640 --> 00:19:46,120
But it has a nice property that we just have to search for individual frames.

239
00:19:46,120 --> 00:19:50,080
So we treat the problem as a back of word approach.

240
00:19:50,080 --> 00:19:55,280
So we just put all these frames inside the index library.

241
00:19:55,280 --> 00:20:00,320
Then we search, and we take all the candidates, and we do a pairwise comparison.

242
00:20:00,320 --> 00:20:05,440
If the pairwise comparison is successful beyond a certain threshold, then it's a match.

243
00:20:05,440 --> 00:20:12,000
And also this you get for free, and it's released along with the PDQ, along with the TMK, PDQF.

244
00:20:12,000 --> 00:20:20,040
All this is available inside the Facebook research GitHub repository.

245
00:20:20,040 --> 00:20:22,840
What do you do once you have all these ashes?

246
00:20:22,840 --> 00:20:28,160
So your platform is computing the ashes, but it's the first time that you see this content.

247
00:20:28,160 --> 00:20:32,600
But perhaps all other actors have already seen this content too.

248
00:20:32,600 --> 00:20:36,760
Well you upload them to the threat exchange platform.

249
00:20:36,760 --> 00:20:42,360
Necmack shares the PDNA ashes, I told you, with all companies that are asking for them.

250
00:20:42,360 --> 00:20:48,120
So can you please tell me where this picture that someone uploaded is a match in Necmack?

251
00:20:48,120 --> 00:20:52,360
So I already know that this is something I should call the law enforcement.

252
00:20:52,360 --> 00:20:59,320
Meta does the equivalent, but for the PDQ, because it has much less friction to adopt

253
00:20:59,320 --> 00:21:02,600
the PDQ compared to the PDNA.

254
00:21:02,600 --> 00:21:06,200
There's a team, the Internet Safety Engineering that builds and operates all this service

255
00:21:06,200 --> 00:21:15,920
where anyone can upload fingerprints, and so you can crowdsource a big graph of matches.

256
00:21:15,920 --> 00:21:23,600
Those rest API to access and post new data, has multi-language clients, uses PDQ, and

257
00:21:23,600 --> 00:21:25,000
users can also download the data.

258
00:21:25,000 --> 00:21:27,800
You are not forced to stay online, stay connected.

259
00:21:27,800 --> 00:21:33,080
You can just request for a dump of the database and you can search it.

260
00:21:33,080 --> 00:21:40,040
And you find all the data and all the APIs at the GitHub page.

261
00:21:40,040 --> 00:21:49,720
In 2020, Facebook also has released its most advanced algorithm to spot similar images,

262
00:21:49,720 --> 00:21:52,720
the SimSearchNet++.

263
00:21:52,720 --> 00:22:01,920
This is an error network, and it is capable of facing adversarial manipulation that the

264
00:22:01,920 --> 00:22:06,040
other embeddings just are not able to.

265
00:22:06,040 --> 00:22:13,240
Unfortunately, SimSearchNet is proprietary, so I cannot really talk about that, but we

266
00:22:13,240 --> 00:22:22,040
have a cousin product, SSCD, the SimSearch Copy Detection, or Similac Research Copy

267
00:22:22,040 --> 00:22:27,560
Detection, which is open source and free, so I can really talk about that.

268
00:22:27,560 --> 00:22:34,640
They are somewhat related in some technological principles, so I can really talk about this.

269
00:22:34,640 --> 00:22:39,480
So this is a PyTorch-based model.

270
00:22:39,480 --> 00:22:46,200
So the problem that this, which is a state-of-the-art product, is trying to solve is what happens

271
00:22:46,200 --> 00:22:54,840
if I take a picture and I put a caption on it, alterating so many pixels everywhere.

272
00:22:54,840 --> 00:23:03,120
A PDQ or a PDNA hash will be altered dramatically, but is there anything we can do to teach

273
00:23:03,120 --> 00:23:10,080
a computer to just ignore all the captions, all the rotations, all the jitters, all the

274
00:23:10,080 --> 00:23:11,680
cropping of the image?

275
00:23:11,680 --> 00:23:12,680
Yes, there is.

276
00:23:12,680 --> 00:23:17,280
A person is able to do that, so we can teach a computer to do that, too.

277
00:23:17,280 --> 00:23:19,200
So models and code are available.

278
00:23:19,200 --> 00:23:24,040
What is now available is the training data that we use to create a model, of course.

279
00:23:24,040 --> 00:23:31,720
For those which are into the deep learning, it's a ResNet50 convolutive neural network,

280
00:23:31,720 --> 00:23:37,320
and the novelty of the approach is that it's based on our MAC vocabularies.

281
00:23:37,320 --> 00:23:43,960
A regional MAC, for those, how many of you know how a convolutive network work?

282
00:23:43,960 --> 00:23:44,960
Raise your hand.

283
00:23:44,960 --> 00:23:45,960
Okay, fine.

284
00:23:45,960 --> 00:23:46,960
Very good.

285
00:23:46,960 --> 00:23:53,800
So it's a network for the others that looks at the image, looks at portions of the image.

286
00:23:53,800 --> 00:23:59,680
Each neuron looks at a different portion, and then they pass what they have understood

287
00:23:59,680 --> 00:24:04,840
to a higher level series of neurons, the higher and the higher and the higher, until the last

288
00:24:04,840 --> 00:24:10,680
layer of the neurons has a very wide overview of the whole picture.

289
00:24:10,680 --> 00:24:17,280
In this case, we are using the maximum activation of all the channels that we have.

290
00:24:17,280 --> 00:24:25,000
So we take note which are the regions of our Carnaut maps for every different channel,

291
00:24:25,000 --> 00:24:28,880
which across all channels have the maximum activation.

292
00:24:28,880 --> 00:24:34,400
If you have ten channels, and that region, across all the different channels, all of

293
00:24:34,400 --> 00:24:39,360
them, you have a maximum activation, that means that that area is an area of interest.

294
00:24:39,360 --> 00:24:45,720
So we use these areas of interest as a word in a vocabulary.

295
00:24:45,720 --> 00:24:52,240
So exactly when you do the cousin similarity search for documents, you take all the words,

296
00:24:52,240 --> 00:24:57,800
you index all the words, you say these documents as these words, so it's like a vector of words,

297
00:24:57,800 --> 00:25:05,120
and then we try to see which are the vectors that have the most words in common and put

298
00:25:05,120 --> 00:25:07,200
in the same place.

299
00:25:07,200 --> 00:25:10,720
We do the same things, but for portions of the image.

300
00:25:10,720 --> 00:25:13,160
So we use the rmax.

301
00:25:13,160 --> 00:25:17,320
The idea is that it's a self-supervised system also.

302
00:25:17,320 --> 00:25:25,280
So it means that it's trained to recognize augmented input, and it's trained to match

303
00:25:25,280 --> 00:25:28,360
an input to its augmented version.

304
00:25:28,360 --> 00:25:32,480
So what we do is that we take the training set, we repeat a lot of augmentation, we add

305
00:25:32,480 --> 00:25:37,920
the captions, the random, we rotate, we flip, we alter the colors.

306
00:25:37,920 --> 00:25:46,640
For example, if you do a one degree of whitening, you make the image brighter, which is you

307
00:25:46,640 --> 00:25:51,600
add plus one to all the pixel in the image, you are altering all the pixels.

308
00:25:51,600 --> 00:25:56,760
But in this case, a PDQ ash is capable of understanding the difference.

309
00:25:56,760 --> 00:26:01,280
There's a very weak form of adversarial attack, because the PDQ just computes the difference

310
00:26:01,280 --> 00:26:03,520
between regions, so it's not going to be fooled.

311
00:26:03,520 --> 00:26:09,320
But you can be much more violent and put just a spot color somewhere, and PDQ is going to

312
00:26:09,320 --> 00:26:10,320
be fooled by that.

313
00:26:10,320 --> 00:26:12,440
Then you do through the CNN.

314
00:26:12,440 --> 00:26:19,520
You do a thing called gem pool, which means you do a generative mean pooling, a generalization

315
00:26:19,520 --> 00:26:22,840
of the average pooling in case you were wondering.

316
00:26:22,840 --> 00:26:30,400
Then you go, and at the end, you use entropy-oriented loss function.

317
00:26:30,400 --> 00:26:39,360
This means that we want to encourage the network to spread the representation of training data

318
00:26:39,360 --> 00:26:46,480
along all different places, because we want to maximize the distance between all the training

319
00:26:46,480 --> 00:26:48,160
examples in the training set.

320
00:26:48,160 --> 00:26:51,200
So you get a nice uniform search space.

321
00:26:51,200 --> 00:26:57,520
At the inference time, you do the same with the CNN, and then you obtain a vector, which

322
00:26:57,520 --> 00:26:59,760
is a representation of an image.

323
00:26:59,760 --> 00:27:06,680
And the idea is that there is a distance that you can compute between the data set of the

324
00:27:06,680 --> 00:27:08,440
reference images.

325
00:27:08,440 --> 00:27:14,680
Of course, you can subtract a background data set that was used generally through augmented

326
00:27:14,680 --> 00:27:15,680
images.

327
00:27:15,680 --> 00:27:20,960
But in this case, what you obtain in the end is that the score of the augmented image

328
00:27:20,960 --> 00:27:27,360
is almost the same of the non-augmented version, because it just learns to ignore the places

329
00:27:27,360 --> 00:27:30,440
which are not organic in the image.

330
00:27:30,440 --> 00:27:34,360
And SSCD is freely available.

331
00:27:34,360 --> 00:27:37,080
You can download that and start playing.

332
00:27:37,080 --> 00:27:42,560
You find both code and models, as I already said, but not the training data.

333
00:27:42,560 --> 00:27:47,120
And by the way, Facebook has also announced an image similarity challenge.

334
00:27:47,120 --> 00:27:52,280
You have to determine whether a query image is a modified copy of any image in a reference

335
00:27:52,280 --> 00:27:53,880
corpus of one million.

336
00:27:53,880 --> 00:28:01,920
This is very similar to the Netflix recommendation challenge, where you had to recommend movies

337
00:28:01,920 --> 00:28:05,480
and you had to beat Netflix algorithm.

338
00:28:05,480 --> 00:28:10,560
And this is the image similarity challenge, and also the meta-IE video similarity challenge,

339
00:28:10,560 --> 00:28:14,520
which is two tracks.

340
00:28:14,520 --> 00:28:18,880
Generate a useful vector representation for a video.

341
00:28:18,880 --> 00:28:23,960
And also try to find a reference video into this very big corpus.

342
00:28:23,960 --> 00:28:26,440
And you don't have to only find a video.

343
00:28:26,440 --> 00:28:36,200
You have to find a clip, so a sub-portion of a video, into a very big corpus.

344
00:28:36,200 --> 00:28:43,440
And last but not least, since the last part of a donor is the tastier one, we have your

345
00:28:43,440 --> 00:28:49,480
turnkey open-source solution that you can install in your own on-premise.

346
00:28:49,480 --> 00:28:51,640
The hushier matcher actioner.

347
00:28:51,640 --> 00:28:57,680
HMA is an open-source turnkey safety solution.

348
00:28:57,680 --> 00:29:03,480
So you just download it, install it, and it starts working right away.

349
00:29:03,480 --> 00:29:09,880
What it does is that it scans the images that you want to push towards it.

350
00:29:09,880 --> 00:29:16,320
It has an index that is updated with all the hashes coming from thread exchange, but also

351
00:29:16,320 --> 00:29:26,160
from yours, and is able to, say, to bind banks verticals of violations.

352
00:29:26,160 --> 00:29:29,680
You might have a non-severe violation or very severe violation.

353
00:29:29,680 --> 00:29:34,680
You might decide that for non-severe violation, you just delete the content and send a warning,

354
00:29:34,680 --> 00:29:41,120
or for high severity violation, you just immediately delete the content, shut down the account

355
00:29:41,120 --> 00:29:45,920
of the poster, and you also signal it to the law enforcement.

356
00:29:45,920 --> 00:29:47,920
You can do that.

357
00:29:47,920 --> 00:29:53,640
And you can configure actions in a backend that are tied to the content that you want

358
00:29:53,640 --> 00:29:58,400
to bank into your HMA platform.

359
00:29:58,400 --> 00:30:03,440
You can pull violating seeds from Facebook thread exchange API, and works on AWS only,

360
00:30:03,440 --> 00:30:12,240
because we wanted to make a very easy-to-use thing, and also something that doesn't really

361
00:30:12,240 --> 00:30:13,760
mix your bill higher.

362
00:30:13,760 --> 00:30:17,600
So we built it on AWS Lambda.

363
00:30:17,600 --> 00:30:22,160
So it doesn't cost anything until it runs, then it runs, spawns a Lambda instance, and

364
00:30:22,160 --> 00:30:27,200
then goes down, and you only pay for the seconds that it actually runs.

365
00:30:27,200 --> 00:30:28,720
But it's very fast.

366
00:30:28,720 --> 00:30:33,480
And there's a Terraform module available thanks to the lovely folks of the Internet Safety

367
00:30:33,480 --> 00:30:34,480
Engineering.

368
00:30:34,480 --> 00:30:37,520
This is how you deploy that.

369
00:30:37,520 --> 00:30:43,320
Your infra, you collocate HMA to your platform.

370
00:30:43,320 --> 00:30:49,240
For example, you might own a platform where people have a chat or people post pictures.

371
00:30:49,240 --> 00:30:55,160
Whenever new content comes, the web server asks the Azure, have you seen this?

372
00:30:55,160 --> 00:30:57,440
And the Azure goes to Matcher.

373
00:30:57,440 --> 00:31:01,000
Matcher goes to the index and says, do I know this?

374
00:31:01,000 --> 00:31:09,200
And in case there's a match, the actioner module will just tell your, you have to define

375
00:31:09,200 --> 00:31:14,880
a callback API in your own platform, like whenever the actioner calls, you are killing

376
00:31:14,880 --> 00:31:17,160
this content in your own backend.

377
00:31:17,160 --> 00:31:24,360
And, of course, you can fetch from external API new content from the fact exchange platform.

378
00:31:24,360 --> 00:31:30,160
So wrapping up, automation is necessary to be effective.

379
00:31:30,160 --> 00:31:35,400
But you will lose precision, of course, because automation doesn't really think.

380
00:31:35,400 --> 00:31:38,920
It just does whatever you have configured blindly.

381
00:31:38,920 --> 00:31:44,400
Human support is always needed for appeals and also to establish the ground through.

382
00:31:44,400 --> 00:31:47,520
So what is actually violating, what is not?

383
00:31:47,520 --> 00:31:52,360
Do expect false positive, because they will happen.

384
00:31:52,360 --> 00:31:58,920
You should put in place an appeal process to allow your users to restore the content.

385
00:31:58,920 --> 00:32:06,480
PDQ, video PDQ, MT5 and SSCD will provide you with a way to obtain compact representation

386
00:32:06,480 --> 00:32:11,440
of high dimensionality content like pictures and videos.

387
00:32:11,440 --> 00:32:17,640
HMA provides you with a turnkey solution that you can install on premise, on your premise,

388
00:32:17,640 --> 00:32:24,280
and search and enforce your integrity policies at your platform.

389
00:32:24,280 --> 00:32:29,080
And thread exchange provides you with a platform for exchanging representation with other

390
00:32:29,080 --> 00:32:34,720
big actors, like, maybe itself, for example.

391
00:32:34,720 --> 00:32:35,720
That was all from me.

392
00:32:35,720 --> 00:32:46,160
Thank you very much for listening.

393
00:32:46,160 --> 00:32:55,120
Any question?

394
00:32:55,120 --> 00:32:58,160
You mentioned it for the challenge, I think?

395
00:32:58,160 --> 00:33:00,440
Oh, louder.

396
00:33:00,440 --> 00:33:05,280
So you mentioned it for the challenge, finding a clip of a video.

397
00:33:05,280 --> 00:33:08,640
Can PDQ do that, actually?

398
00:33:08,640 --> 00:33:11,240
You can't hear me.

399
00:33:11,240 --> 00:33:19,280
So can PDQ find clips of videos?

400
00:33:19,280 --> 00:33:20,880
That's my question, actually.

401
00:33:20,880 --> 00:33:27,920
So you should, you say, perhaps I heard about YouTube, whether it is something that already

402
00:33:27,920 --> 00:33:28,920
does.

403
00:33:28,920 --> 00:33:41,200
Like, if the challenge is to find the clips of videos, yeah, in general it's possible

404
00:33:41,200 --> 00:33:46,920
of course, and the video PDQ algorithms will ask every frame.

405
00:33:46,920 --> 00:33:54,560
So in case you send a very small sub portion of a video, you will have, like, 100 frames,

406
00:33:54,560 --> 00:33:58,600
for example, then these 100 frames will be treated as a bag of words.

407
00:33:58,600 --> 00:34:03,920
You search the index, you find the video that contained all of these words.

408
00:34:03,920 --> 00:34:10,880
So you have a match of all your query frames inside the index at the very long video that

409
00:34:10,880 --> 00:34:11,880
has it.

410
00:34:11,880 --> 00:34:13,760
And so it's a match.

411
00:34:13,760 --> 00:34:14,760
That's how we do.

412
00:34:14,760 --> 00:34:18,520
Of course, there are more clever ways to do that.

413
00:34:18,520 --> 00:34:19,520
Thanks.

414
00:34:19,520 --> 00:34:20,520
Hello.

415
00:34:20,520 --> 00:34:28,160
Not a technical question, but let's see.

416
00:34:28,160 --> 00:34:35,720
I was thinking that if you're using such a system to try to prevent digital crimes and

417
00:34:35,720 --> 00:34:37,760
such things like that.

418
00:34:37,760 --> 00:34:47,680
From an ethical perspective, I was just wondering, I suppose you have such images to compare them,

419
00:34:47,680 --> 00:34:49,680
and how do you process those?

420
00:34:49,680 --> 00:34:53,160
How do you make the decisions?

421
00:34:53,160 --> 00:34:57,520
So I repeat the question.

422
00:34:57,520 --> 00:35:02,400
From the ethical perspective, the idea is that, of course, we have to see the images

423
00:35:02,400 --> 00:35:06,080
in order to be able to know what's happening, right?

424
00:35:06,080 --> 00:35:13,160
Yeah, see, and of course you have to save them and process them, and how do you handle

425
00:35:13,160 --> 00:35:14,560
this?

426
00:35:14,560 --> 00:35:20,000
So this is not the kind of question that I really can answer because it is related to

427
00:35:20,000 --> 00:35:22,520
internal procedures.

428
00:35:22,520 --> 00:35:30,120
But if we have to compute the fingerprint of an image, there must be a one second in

429
00:35:30,120 --> 00:35:34,280
which the image is on our servers.

430
00:35:34,280 --> 00:35:43,400
It is, since the agency is like Necmac, they share ashes, so you might have an ash for

431
00:35:43,400 --> 00:35:48,880
which you don't have a picture, and you have to trust that this ash is coming from a trusted

432
00:35:48,880 --> 00:35:54,520
source that has already vetted whether this ash is nasty stuff or not.

433
00:35:54,520 --> 00:36:01,400
That's how we actually avoid sanctioning heavily innocent people.

434
00:36:01,400 --> 00:36:05,600
So there is a collaboration with the trusted entities for this.

435
00:36:05,600 --> 00:36:10,600
When you receive those from an external agent, if those images are on your platform, you

436
00:36:10,600 --> 00:36:14,560
already know what you've seen.

437
00:36:14,560 --> 00:36:16,200
Thank you.

438
00:36:16,200 --> 00:36:19,120
Can you hear me despite the mask?

439
00:36:19,120 --> 00:36:20,120
Can you hear me?

440
00:36:20,120 --> 00:36:22,240
Yeah, thank you.

441
00:36:22,240 --> 00:36:27,600
So I have a question, but first I have a thanks because I have worked in this kind of thing

442
00:36:27,600 --> 00:36:36,560
and Necmac doesn't share any useful data, IWF doesn't share any useful data, Farrows

443
00:36:36,560 --> 00:36:41,320
doesn't share any useful data, so I will definitely take a look at the threat exchange

444
00:36:41,320 --> 00:36:46,000
platform and hope that it's much more useful, and thanks for that.

445
00:36:46,000 --> 00:36:49,440
Now I have a question anyway.

446
00:36:49,440 --> 00:36:56,600
If I was an attacker, I could download data from the threat exchange platform and try

447
00:36:56,600 --> 00:37:03,360
and run as many filters automatically until I find something that is not matched by PDQ,

448
00:37:03,360 --> 00:37:06,000
video PDQ, et cetera.

449
00:37:06,000 --> 00:37:07,440
What's the way to counter that?

450
00:37:07,440 --> 00:37:13,200
Oh, you're asking whether adversarial attacks are possible on PDQ?

451
00:37:13,200 --> 00:37:19,480
Yeah, of course, PDQ is a very naive algorithm that just detects the patches of colors.

452
00:37:19,480 --> 00:37:24,440
It is actually possible to create adversarial attacks.

453
00:37:24,440 --> 00:37:34,400
Just if you think that you alter many pixels in the image and perceptually, for us, doesn't

454
00:37:34,400 --> 00:37:43,560
change anything, but you might end up changing the most relevant pictures for the DCT algorithm.

455
00:37:43,560 --> 00:37:51,200
I will create a completely different ashing in the end.

456
00:37:51,200 --> 00:37:58,240
Also, someone has demonstrated an attack, a reverse engineering attack on photo DNA,

457
00:37:58,240 --> 00:38:09,520
like from the project it's called ribosome, and it's a neural network that from a hash

458
00:38:09,520 --> 00:38:19,240
reconstructs a very blurry picture, so it is actually possible to do that, but PDQ is

459
00:38:19,240 --> 00:38:21,520
a very simple and fast algorithm.

460
00:38:21,520 --> 00:38:29,960
If you really want to combat seriously adversarial engineering the things, you need neural networks

461
00:38:29,960 --> 00:38:37,000
like SSCD because it contains so many relations to different parts of the images, it's much

462
00:38:37,000 --> 00:38:38,000
harder to fool.

463
00:38:38,000 --> 00:38:41,480
I'm not saying it's not impossible because, of course, it's possible.

464
00:38:41,480 --> 00:38:47,760
Sooner or later, someone will find a way, but it's the usual arms race between attackers

465
00:38:47,760 --> 00:38:50,960
and defenders, and it's no exception.

466
00:38:50,960 --> 00:38:52,760
Thank you for your question.

467
00:38:52,760 --> 00:38:53,760
Hello.

468
00:38:53,760 --> 00:38:54,760
Hi.

469
00:38:54,760 --> 00:38:57,760
First, thank you for the presentation.

470
00:38:57,760 --> 00:38:59,960
I think it's a very interesting topic.

471
00:38:59,960 --> 00:39:07,600
I wanted to link it to because it's been a bit of a buzz the past few weeks, the generative

472
00:39:07,600 --> 00:39:13,200
AI, especially chat GPT, was wondering if when you use that kind of algorithm and you scan

473
00:39:13,200 --> 00:39:18,400
an image, detects something, is there a level of confidence attached to the result and can

474
00:39:18,400 --> 00:39:21,760
you detect when an image is potentially a fake or...

475
00:39:21,760 --> 00:39:28,480
There's a lot of time because there's an echo, so I cannot really... Can you do it louder

476
00:39:28,480 --> 00:39:29,480
please?

477
00:39:29,480 --> 00:39:31,880
It's hard to understand from here.

478
00:39:31,880 --> 00:39:32,880
Hello.

479
00:39:32,880 --> 00:39:33,880
Okay.

480
00:39:33,880 --> 00:39:34,880
Is it better?

481
00:39:34,880 --> 00:39:35,880
Okay.

482
00:39:35,880 --> 00:39:42,400
Yeah, so I said thank you, but I wanted to link to generative AI and I was asking, so

483
00:39:42,400 --> 00:39:49,080
when you run that kind of algorithm to detect violence or child abuse or anything else,

484
00:39:49,080 --> 00:39:56,560
can you also attach a level of confidence in the response to define whether it's a potentially

485
00:39:56,560 --> 00:40:01,520
fake picture or is there an extension to the algorithm where you can link with the generative

486
00:40:01,520 --> 00:40:04,600
AI?

487
00:40:04,600 --> 00:40:08,640
I'm not sure about the answer.

488
00:40:08,640 --> 00:40:16,720
Sorry, we can go for a beer and I can explain more details and let's see.

489
00:40:16,720 --> 00:40:20,400
Yeah, you have a question.

490
00:40:20,400 --> 00:40:21,900
Hi.

491
00:40:21,900 --> 00:40:22,900
Thank you for the talk.

492
00:40:22,900 --> 00:40:23,900
It was very interesting.

493
00:40:23,900 --> 00:40:24,900
Thank you.

494
00:40:24,900 --> 00:40:25,900
One more question also.

495
00:40:25,900 --> 00:40:28,880
Do you run SSCD in production as well?

496
00:40:28,880 --> 00:40:31,240
The deep learning network?

497
00:40:31,240 --> 00:40:35,200
If we're using SSCD in production, can't I reply to this question?

498
00:40:35,200 --> 00:40:36,200
Okay.

499
00:40:36,200 --> 00:40:41,200
We're using search net plus plus.

500
00:40:41,200 --> 00:40:42,200
Yes.

501
00:40:42,200 --> 00:40:47,600
We use this other one because we have written a blog post about this, so I can confirm that

502
00:40:47,600 --> 00:40:49,720
we use the same search net plus plus.

503
00:40:49,720 --> 00:40:50,720
Okay.

504
00:40:50,720 --> 00:40:56,560
I cannot nor confirm or deny about SSCD, but those are related technologies, so I can

505
00:40:56,560 --> 00:40:57,560
talk.

506
00:40:57,560 --> 00:40:58,560
That's okay.

507
00:40:58,560 --> 00:41:02,240
What does the production stack for SIM search nets plus plus look like?

508
00:41:02,240 --> 00:41:03,240
How do you serve it?

509
00:41:03,240 --> 00:41:06,240
It must be pretty hard to deal with the GPUs and everything.

510
00:41:06,240 --> 00:41:07,680
This is not a question that unfortunately...

511
00:41:07,680 --> 00:41:08,680
Okay.

512
00:41:08,680 --> 00:41:09,680
I'm sorry.

513
00:41:09,680 --> 00:41:10,680
I cannot talk about production setups.

514
00:41:10,680 --> 00:41:12,680
I'm sorry.

515
00:41:12,680 --> 00:41:14,560
Okay.

516
00:41:14,560 --> 00:41:15,560
Any question nearby?

517
00:41:15,560 --> 00:41:16,560
Thank you.

518
00:41:16,560 --> 00:41:21,560
But of course, you can imagine that we do not operate in the vacuum.

519
00:41:21,560 --> 00:41:30,320
So if you can think about how we serve results from a neural network, it is something perhaps

520
00:41:30,320 --> 00:41:42,360
similar to what would you do if you would have to put behind an API a model?

521
00:41:42,360 --> 00:41:44,680
So I kind of have two questions.

522
00:41:44,680 --> 00:41:49,880
The first question is to what extent do...

523
00:41:49,880 --> 00:41:53,880
So I think there are potentially two problems.

524
00:41:53,880 --> 00:41:59,480
Intentional mismatches and unintentional mismatches.

525
00:41:59,480 --> 00:42:05,280
So situations where perhaps an image has been recompressed or has been cropped or is perhaps

526
00:42:05,280 --> 00:42:10,600
another image of the same situation versus situations where people have deliberately

527
00:42:10,600 --> 00:42:14,640
deformed the image to try and get around these kind of systems.

528
00:42:14,640 --> 00:42:15,640
So...

529
00:42:15,640 --> 00:42:21,360
Do you have any idea of how performant it is against the two scenarios of either accidental

530
00:42:21,360 --> 00:42:26,000
or unintentional mismatches versus intentionally trying to avoid it?

531
00:42:26,000 --> 00:42:32,200
So it is, of course, possible to have unintentional mismatches.

532
00:42:32,200 --> 00:42:42,400
And I've seen images that were adversarial engineered to give the same embedding.

533
00:42:42,400 --> 00:42:48,480
Those are absolutely possible, again, in PDQ, PDNA and all the perceptual hashing, which

534
00:42:48,480 --> 00:42:50,840
is just a mathematical transformation.

535
00:42:50,840 --> 00:42:57,280
You just have to find a way where the input seems the same to the algorithm.

536
00:42:57,280 --> 00:43:02,520
For the neural network things, it depends.

537
00:43:02,520 --> 00:43:03,920
You can study the code.

538
00:43:03,920 --> 00:43:06,320
You can study how it's done.

539
00:43:06,320 --> 00:43:07,320
If you can...

540
00:43:07,320 --> 00:43:15,280
It is absolutely possible sooner or later because the adversarial attacker on combinets

541
00:43:15,280 --> 00:43:16,720
are a reality.

542
00:43:16,720 --> 00:43:18,200
So it's absolutely possible.

543
00:43:18,200 --> 00:43:26,320
I've seen some mismatches, but usually to perceptual hashes.

544
00:43:26,320 --> 00:43:32,040
Usually the more refined the technique, the harder it is to attack, of course, otherwise

545
00:43:32,040 --> 00:43:36,480
we just will stay with MD5 because it will be enough.

546
00:43:36,480 --> 00:43:37,480
Crops.

547
00:43:37,480 --> 00:43:46,160
PDQ is resistant to crops, SSCD is very resistant to crops.

548
00:43:46,160 --> 00:43:52,680
If you have rotations, I believe also PDQ is resistant to rotations like flips, but

549
00:43:52,680 --> 00:43:58,280
you cannot ask much more than that.

550
00:43:58,280 --> 00:43:59,280
Other questions?

551
00:43:59,280 --> 00:44:00,280
Yeah.

552
00:44:00,280 --> 00:44:08,680
Do you have any information about speed difference between SSCD and PDQ?

553
00:44:08,680 --> 00:44:16,880
So the question is whether I have some speed benchmarks for the difference of performance

554
00:44:16,880 --> 00:44:21,920
between PDQ and SSCD at inference time.

555
00:44:21,920 --> 00:44:28,640
PDQ is faster than your time to read the image from disk.

556
00:44:28,640 --> 00:44:30,520
So it's negligible.

557
00:44:30,520 --> 00:44:31,520
It will just compute.

558
00:44:31,520 --> 00:44:33,920
It's a mathematical transformation on the pixel.

559
00:44:33,920 --> 00:44:40,840
The neural network requires a dedicated hardware, if you do that on CPU, it will take seconds

560
00:44:40,840 --> 00:44:44,040
also because the model I think is big enough.

561
00:44:44,040 --> 00:44:49,720
It's not as big as GPT, but it's a 50 level CNET.

562
00:44:49,720 --> 00:44:56,480
So it's of course lower and requires dedicated hardware, but it's more precise.

563
00:44:56,480 --> 00:45:02,760
It just finds, SSCD finds anything that PDQ is able to find and much more.

564
00:45:02,760 --> 00:45:11,600
So in case if you are very curious about, sorry, if you are very conscious about, I

565
00:45:11,600 --> 00:45:17,120
have to scan this stuff just to make sure they don't come from a ill source.

566
00:45:17,120 --> 00:45:21,840
You might want to set up an async process that will take more, but will just batch process

567
00:45:21,840 --> 00:45:22,840
all your stuff.

568
00:45:22,840 --> 00:45:30,240
If you need a super fast thing, PDQ will not really wait over your server.

569
00:45:30,240 --> 00:45:33,720
Thank you.

570
00:45:33,720 --> 00:45:34,720
Any other question?

571
00:45:34,720 --> 00:45:35,720
Hi.

572
00:45:35,720 --> 00:45:44,880
First of all, great question from my former colleague David, I think, down there.

573
00:45:44,880 --> 00:45:47,240
Not even looking this way.

574
00:45:47,240 --> 00:45:53,040
But what happens if you get a false positive match?

575
00:45:53,040 --> 00:45:58,640
How do you disregard that in the future without potentially disregarding a real match?

576
00:45:58,640 --> 00:46:03,920
So if we get a false positive match, how do we do to restore?

577
00:46:03,920 --> 00:46:06,800
Yeah, how do you restore and keep it there?

578
00:46:06,800 --> 00:46:10,000
With HMA, you mean or in MEDA all day?

579
00:46:10,000 --> 00:46:11,400
Just anywhere, like as a concept.

580
00:46:11,400 --> 00:46:14,400
So in MEDA, I cannot really say.

581
00:46:14,400 --> 00:46:21,240
With the Hasher Matcher Actioner, you have the, you should provide a capability to your

582
00:46:21,240 --> 00:46:27,400
own platform for which you are soft deleting the image, because you have to provide away

583
00:46:27,400 --> 00:46:34,560
an API in your platform that HMA will call on, where you say, soft delete this picture,

584
00:46:34,560 --> 00:46:39,560
so make it unavailable, but do not really delete it in case you want to appeal.

585
00:46:39,560 --> 00:46:47,200
So you need to provide an undelete, and unsoft delete, and soft delete capabilities.

586
00:46:47,200 --> 00:46:54,600
This is the simplest and most effective way to deal with false positive in case, whoops,

587
00:46:54,600 --> 00:46:57,520
I did a mistake, I want to restore the content.

588
00:46:57,520 --> 00:47:02,520
Sure, but if you have an image that someone wants to upload, say it's a popular image

589
00:47:02,520 --> 00:47:09,240
that a lot of people are going to upload, but it matches a pattern of another bad image,

590
00:47:09,240 --> 00:47:15,840
can you, is there a good way to make a more precise hash and exclude that, and say this

591
00:47:15,840 --> 00:47:20,040
one is a false positive, it doesn't match what you think it does, so you don't have

592
00:47:20,040 --> 00:47:21,040
to keep undoing.

593
00:47:21,040 --> 00:47:25,800
Okay, if I understand correctly, if the image is popular, so we have many examples, and

594
00:47:25,800 --> 00:47:32,400
we have many examples of an image which is not bad, and then comes a bad image, whether

595
00:47:32,400 --> 00:47:38,600
we can use the fact that it's very widespread to augment our position, is this the question?

596
00:47:38,600 --> 00:47:47,360
Okay, well, really, there's nothing in this presentation that says these, because once

597
00:47:47,360 --> 00:47:52,480
you train, the network is trained, you start serving, and the network will give you the

598
00:47:52,480 --> 00:47:56,200
same answers to the same question, to the same query.

599
00:47:56,200 --> 00:48:02,160
PDQ or other mathematical algorithm, perceptual algorithm, it's just a mathematical function

600
00:48:02,160 --> 00:48:05,000
so it will not change, there's nothing to train.

601
00:48:05,000 --> 00:48:14,320
So to change a deficiency of your model, you have to retrain, and you can do a better retraining

602
00:48:14,320 --> 00:48:20,760
here, and sometimes models are retrained as anything which is still under maintenance.

603
00:48:20,760 --> 00:48:25,600
For example, we get new data, for example, and we might want to retrain as any other

604
00:48:25,600 --> 00:48:31,440
model also for the spam filters, it's the same.

605
00:48:31,440 --> 00:48:37,000
Do we have more room for questions?

606
00:48:37,000 --> 00:48:38,000
I think it's done.

607
00:48:38,000 --> 00:48:46,120
Thank you so much, you'll be a wonderful audience.

